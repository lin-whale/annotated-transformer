{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import log_softmax, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_repr(self):\n",
    "    return \"{} {}\".format(self.size(), origin_repr(self))\n",
    "\n",
    "origin_repr = torch.Tensor.__repr__\n",
    "torch.Tensor.__repr__ = custom_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "    \n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        None\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(\n",
    "            self.encode(src, src_mask), src_mask,\n",
    "            tgt, tgt_mask\n",
    "        )\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "# class EncoderDecoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A standard Encoder-Decoder architecture. Base for this and many\n",
    "#     other models.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "#         super(EncoderDecoder, self).__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.src_embed = src_embed\n",
    "#         self.tgt_embed = tgt_embed\n",
    "#         self.generator = generator\n",
    "\n",
    "#     def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "#         \"Take in and process masked src and target sequences.\"\n",
    "#         return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "#     def encode(self, src, src_mask):\n",
    "#         return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "#     def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "#         return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "def clones(layer, N):\n",
    "    return nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.layernorm = nn.LayerNorm(layer.d_model)\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        x = src\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return self.layernorm(x)\n",
    "\n",
    "class SubLayerConnection(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, layer):\n",
    "        out = layer(self.layernorm(x))\n",
    "        return self.dropout(out) + x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, attn, ffn, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "        self.d_model = d_model\n",
    "        self.sublayers = clones(SubLayerConnection(d_model, dropout), 2)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        x = self.sublayers[0](src, lambda x: self.attn(x, x, x, src_mask))\n",
    "        x = self.sublayers[1](x, self.ffn)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.layernorm = nn.LayerNorm(layer.d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, src_mask, tgt_mask):\n",
    "        x = tgt\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.layernorm(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, src_attn, ffn, dropout):\n",
    "        super().__init__()\n",
    "        self.src_attn = src_attn\n",
    "        self.self_attn = self_attn\n",
    "        self.ffn = ffn\n",
    "        self.d_model = d_model\n",
    "        self.sublayers = clones(SubLayerConnection(d_model, dropout), 3)\n",
    "\n",
    "    def forward(self, tgt, memory, src_mask, tgt_mask):\n",
    "        x = self.sublayers[0](tgt, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayers[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
    "        x = self.sublayers[2](x, self.ffn)\n",
    "        return x\n",
    "\n",
    "def subsequent_mask(seq_len):\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.int8))\n",
    "    mask = mask.unsqueeze(0)\n",
    "    return mask == 1\n",
    "\n",
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "    d_k =  q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    out = torch.matmul(p_attn, v)\n",
    "    return out\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=None):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.dk = d_model // h\n",
    "        self.h = h\n",
    "        if dropout is not None:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "\n",
    "    def forward(self, q, k, v, mask: torch.Tensor):\n",
    "        batch_size = q.size()[0]\n",
    "        q, k, v = [\n",
    "            lin(x).reshape(batch_size, -1, self.h, self.dk).transpose(1, 2) \n",
    "            for x, lin in zip([q, k, v], self.linears)\n",
    "        ]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        out = attention(q, k, v, mask)\n",
    "        \n",
    "        out = out.transpose(1, 2).reshape(batch_size, -1, self.h * self.dk)\n",
    "        out = self.linears[-1](out)\n",
    "        del q, k, v\n",
    "        return out\n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.lut = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        pos_embed = torch.ones(max_len, d_model)\n",
    "        div_item = torch.exp(-math.log(10000) * torch.arange(0, d_model, 2) / d_model)\n",
    "        pos = torch.arange(0, max_len, 1).unsqueeze(1)\n",
    "        pos_embed[:, 0::2] = torch.sin(pos * div_item)\n",
    "        pos_embed[:, 1::2] = torch.cos(pos * div_item)\n",
    "        self.register_buffer('pos_embed', pos_embed.detach().clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pos_embed[:seq_len]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ffn, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x).relu()\n",
    "        return self.linear2(self.dropout(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        N=6,\n",
    "        d_model=512,\n",
    "        d_ffn=2048,\n",
    "        dropout=0.1,\n",
    "        h=8,\n",
    "        max_len=128\n",
    ") -> EncoderDecoder:\n",
    "    from copy import deepcopy as c\n",
    "    src_embed = nn.Sequential(Embeddings(d_model, vocab_src), PositionalEncoding(d_model, dropout, max_len))\n",
    "    tgt_embed = nn.Sequential(Embeddings(d_model, vocab_tgt), PositionalEncoding(d_model, dropout, max_len))\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ffn = PositionwiseFeedForward(d_model, d_ffn)\n",
    "    generator = Generator(d_model, vocab_tgt)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ffn), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ffn), dropout), N),\n",
    "        src_embed,\n",
    "        tgt_embed,\n",
    "        generator\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model: EncoderDecoder, src, src_mask, max_len, start_index):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    tgt = torch.tensor([[start_index]], dtype=torch.int64).to(src.detach())\n",
    "    for _ in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, tgt, subsequent_mask(tgt.size(1)).to(src.detach()))\n",
    "        out = model.generator(out)\n",
    "        last_embed = out[0][-1]\n",
    "        _, nxt = torch.max(last_embed, dim=-1)\n",
    "        tgt = torch.concat(\n",
    "            [tgt, torch.tensor([[nxt.item()]])],\n",
    "            dim=1\n",
    "        )\n",
    "    # print(tgt)\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10]) tensor([[1, 5, 7, 8, 5, 7, 8, 5, 7, 8]])\n",
      "torch.Size([1, 10]) tensor([[ 1,  3, 10, 10, 10, 10, 10, 10, 10, 10]])\n",
      "torch.Size([1, 10]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([1, 10]) tensor([[1, 5, 6, 6, 6, 6, 6, 6, 6, 6]])\n",
      "torch.Size([1, 10]) tensor([[1, 8, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "torch.Size([1, 10]) tensor([[1, 4, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "torch.Size([1, 10]) tensor([[1, 0, 9, 3, 9, 3, 9, 1, 1, 1]])\n",
      "torch.Size([1, 10]) tensor([[1, 2, 2, 6, 2, 6, 2, 6, 2, 6]])\n",
      "torch.Size([1, 10]) tensor([[1, 3, 0, 7, 3, 8, 1, 3, 8, 0]])\n",
      "torch.Size([1, 10]) tensor([[1, 9, 7, 9, 7, 2, 8, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "def model_inference():\n",
    "    V = 11\n",
    "    model = make_model(V, V, 2)\n",
    "    src = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=torch.int64)\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "    model.eval()\n",
    "    greedy_decode(model, src, src_mask, 10, 1)\n",
    "\n",
    "for _ in range(10):\n",
    "    model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, tgt, padding_idx=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != padding_idx).unsqueeze(1).to(src.detach())\n",
    "        self.tgt = tgt[:, :-1].detach().clone()\n",
    "        self.tgt_y = tgt[:, 1:].detach().clone()\n",
    "        self.tgt_mask = (self.tgt_y != padding_idx).unsqueeze(-2) & subsequent_mask(self.tgt_y.size(1)).to(src.detach())\n",
    "        # self.tgt_mask = self.tgt_mask.to(src.detach())\n",
    "        self.n_tokens = (self.tgt_y != padding_idx).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, padding_idx=0, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.pad_idx = padding_idx\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred: Tensor, label: Tensor):\n",
    "        true_dist = pred.detach().clone()\n",
    "        categroy_size = pred.size(-1)\n",
    "        true_dist.fill_(self.smoothing / (categroy_size-2))\n",
    "        # true_dist.index_fill_(1, torch.tensor(self.pad_idx).to(pred), 0)\n",
    "        true_dist[:, self.pad_idx] = 0\n",
    "        true_dist.scatter_(1, label.unsqueeze(1), 1 - self.smoothing)\n",
    "        mask = torch.nonzero(label == self.pad_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        return self.criterion(pred, true_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLoss:\n",
    "    def __init__(self, crit, generator):\n",
    "        self.crit = crit\n",
    "        self.generator = generator\n",
    "\n",
    "    def __call__(self, x, label, norm):\n",
    "        logit = self.generator(x)\n",
    "        sloss = self.crit(\n",
    "            logit.reshape(-1, logit.size(-1)),\n",
    "            label.reshape(-1)\n",
    "        ) / norm\n",
    "        return sloss.detach() * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(vocab, batch_size, seq_len, batches):\n",
    "    for _ in range(batches):\n",
    "        data = torch.randint(1, vocab, (batch_size, seq_len)).to(torch.int64)\n",
    "        data[:, 0] = 1\n",
    "        src = data.clone()\n",
    "        tgt = data.clone()\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_epoch(\n",
    "        data_iter,\n",
    "        model: EncoderDecoder,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        loss_compute,\n",
    "        accum_interval,\n",
    "        mode='train'\n",
    "):\n",
    "    accum_steps = 0\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.n_tokens)\n",
    "        total_loss += loss.item()\n",
    "        if 'train' in mode:\n",
    "            total_tokens += batch.n_tokens\n",
    "            loss_node.backward()\n",
    "            if i % accum_interval == 0:\n",
    "                accum_steps += 1\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        if i % 40 == 0 and 'train' in mode:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            end_time = time.time()\n",
    "            print(\n",
    "                \"Epoch Step: {:6d} | Accumulation Step: {:3d} | Loss: {:6.2f} | Tokens / Sec: {:7.1f} | Learning Rate: {:6.1e}\".format(\n",
    "                    i, accum_steps, loss_node.detach().item(), total_tokens / (end_time - start_time), lr\n",
    "                )\n",
    "            )\n",
    "            start_time = time.time()\n",
    "        del loss, loss_node\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, warmup, d_model, factor=1.0):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return d_model ** -0.5 * factor * min(step ** -0.5, warmup ** -1.5 * step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    V = 11\n",
    "    model = make_model(V, V, 2)\n",
    "    d_model = 512\n",
    "    epochs = 20\n",
    "    batch_size = 80\n",
    "    batches = 20\n",
    "    seq_len = 10\n",
    "    warm_up = 400\n",
    "    accum_interval = 1\n",
    "    loss_compute = SimpleLoss(\n",
    "        LabelSmoothing(0, 0.0),\n",
    "        model.generator\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: rate(x, warm_up, d_model, 1))\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, seq_len, batches),\n",
    "            model,\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            loss_compute,\n",
    "            accum_interval,\n",
    "            'train'\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, seq_len, batches),\n",
    "            model,\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            loss_compute,\n",
    "            accum_interval,\n",
    "            'eval'\n",
    "        )\n",
    "\n",
    "    src = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=torch.int64)\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "    model.eval()\n",
    "    greedy_decode(model, src, src_mask, 10, 1)\n",
    "\n",
    "# train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from torchtext import datasets\n",
    "from torchtext.datasets import multi30k\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
    "\n",
    "multi30k.MD5[\"train\"] = \"20140d013d05dd9a72dfde46478663ba05737ce983f478f960c1123c6671be5e\"\n",
    "multi30k.MD5[\"valid\"] = \"a7aa20e9ebd5ba5adce7909498b94410996040857154dab029851af3a866da8c\"\n",
    "multi30k.MD5[\"test\"] = \"6d1ca1dba99e2c5dd54cae1226ff11c2551e6ce63527ebb072a1f70f72a5cd36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer():\n",
    "    try:\n",
    "        spacy_de, spacy_en = spacy.load('de_core_news_sm'), spacy.load('en_core_web_sm')\n",
    "    except IOError:\n",
    "        os.system('python -m spacy download en_core_web_sm')\n",
    "        os.system('python -m spacy download de_core_news_sm')\n",
    "        spacy_de, spacy_en = spacy.load('de_core_news_sm'), spacy.load('en_core_web_sm')\n",
    "    \n",
    "    return spacy_de, spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de, spacy_en = load_tokenizer()\n",
    "\n",
    "def tokenize(tokenizer, text):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "def yield_tokens(dataset, tokenizer, index):\n",
    "    for item in dataset:\n",
    "        yield tokenizer(item[index])\n",
    "\n",
    "def build_vocab():\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(spacy_de, text)\n",
    "    \n",
    "    def tokenize_en(text):\n",
    "        return tokenize(spacy_en, text)\n",
    "\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "\n",
    "    vocab_de = build_vocab_from_iterator(\n",
    "        iterator=yield_tokens(train+val+test, tokenize_de, 0),\n",
    "        min_freq=1,\n",
    "        specials=['<s>', '</s>', '<blank>', '<unk>']\n",
    "    )\n",
    "\n",
    "    vocab_en = build_vocab_from_iterator(\n",
    "        iterator=yield_tokens(train+val+test, tokenize_en, 1),\n",
    "        min_freq=1,\n",
    "        specials=['<s>', '</s>', '<blank>', '<unk>']\n",
    "    )\n",
    "    return vocab_de, vocab_en\n",
    "\n",
    "def load_vocab():\n",
    "    if os.path.exists('vocab.pt'):\n",
    "        vocab_de, vocab_en = torch.load('vocab.pt')\n",
    "    else:\n",
    "        vocab_de, vocab_en = build_vocab()\n",
    "        torch.save((vocab_de, vocab_en), 'vocab.pt')\n",
    "    return vocab_de, vocab_en\n",
    "\n",
    "vocab_de, vocab_en = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(type(spacy_en('hello_world')))\n",
    "print(type(spacy_en('hello_world')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "        batch,\n",
    "        pipline_de,\n",
    "        pipline_en,\n",
    "        vocab_de,\n",
    "        vocab_en,\n",
    "        device,\n",
    "        max_len,\n",
    "):\n",
    "    start_index, end_index = vocab_de['<s>'], vocab_de['</s>']\n",
    "    padding_idx = vocab_de['<blank>']\n",
    "    src_list, tgt_list = [], []\n",
    "    for from_to_tuple in batch:\n",
    "        src, tgt = from_to_tuple\n",
    "        src = torch.concat(\n",
    "            [\n",
    "                torch.tensor([start_index], device=device, dtype=torch.int64),\n",
    "                torch.tensor(\n",
    "                    vocab_de(pipline_de(src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device\n",
    "                ),\n",
    "                torch.tensor([end_index], device=device, dtype=torch.int64)\n",
    "            ],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        tgt = torch.concat(\n",
    "            [\n",
    "                torch.tensor([start_index], device=device, dtype=torch.int64),\n",
    "                torch.tensor(\n",
    "                    vocab_en(pipline_en(tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device\n",
    "                ),\n",
    "                torch.tensor([end_index], device=device, dtype=torch.int64)\n",
    "            ],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        src = pad(\n",
    "            src,\n",
    "            [0, max_len-src.size(0)],\n",
    "            value=padding_idx\n",
    "        )\n",
    "\n",
    "        tgt = pad(\n",
    "            tgt,\n",
    "            [0, max_len-tgt.size(0)],\n",
    "            value=padding_idx\n",
    "        )\n",
    "\n",
    "        src_list.append(src)\n",
    "        tgt_list.append(tgt)\n",
    "    return (torch.stack(src_list, dim=0), torch.stack(tgt_list, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(\n",
    "        config,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        vocab_de,\n",
    "        vocab_en,\n",
    "        device,\n",
    "):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(spacy_de, text)\n",
    "    \n",
    "    def tokenize_en(text):\n",
    "        return tokenize(spacy_en, text)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_de,\n",
    "            vocab_en,\n",
    "            device,\n",
    "            config['max_seqlen']\n",
    "        )\n",
    "\n",
    "    train, val, test = datasets.Multi30k(language_pair=('de', 'en'))\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "        device,\n",
    "        config,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        vocab_de,\n",
    "        vocab_en\n",
    "):\n",
    "    d_model = 512\n",
    "    padding_idx = vocab_de['<unk>']\n",
    "    model = make_model(len(vocab_de), len(vocab_en), max_len=5000)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = LabelSmoothing(padding_idx, 0.1)\n",
    "    criterion.to(device)\n",
    "\n",
    "    train_dataloader, eval_dataloader = create_dataloader(\n",
    "        config,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        vocab_de,\n",
    "        vocab_en,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config['base_lr'], betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda = lambda step: rate(step, config['warmup'], d_model)\n",
    "    )\n",
    "\n",
    "    loss_compute = SimpleLoss(criterion, model.generator)\n",
    "\n",
    "    for i in range(config['epochs']):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            (Batch(batch[0], batch[1], padding_idx) for batch in train_dataloader),\n",
    "            model,\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            loss_compute,\n",
    "            accum_interval=config['accum_interval'],\n",
    "            mode='train'\n",
    "        )\n",
    "\n",
    "        torch.save(model.state_dict(), '{}_{:2d}.pt'.format(config['model_prefix'], i))\n",
    "\n",
    "        model.eval()\n",
    "        run_epoch(\n",
    "            (Batch(batch[0], batch[1], padding_idx) for batch in eval_dataloader),\n",
    "            model,\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            loss_compute,\n",
    "            1,\n",
    "            'eval'\n",
    "        )\n",
    "    torch.save(model.state_dict(), '{}_final.pt'.format(config['model_prefix']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model():\n",
    "    config = {\n",
    "        'epochs': 8,\n",
    "        'model_prefix': 'multi30k',\n",
    "        'accum_interval': 10,\n",
    "        'batch_size': 32,\n",
    "        'warmup': 3000,\n",
    "        'base_lr': 1.0,\n",
    "        'max_seqlen': 72,\n",
    "    }\n",
    "\n",
    "    if not os.path.exists('{}_final.pt'.format(config['model_prefix'])):\n",
    "        train_worker(0, config, spacy_de, spacy_en, vocab_de, vocab_en)\n",
    "    model = make_model(len(vocab_de), len(vocab_en), max_len=5000)\n",
    "    model.load_state_dict(torch.load('{}_final.pt'.format(config['model_prefix'])))\n",
    "    return model\n",
    "\n",
    "load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data ...\n",
      "Loading Trained Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zme/anaconda3/envs/Transformer/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/home/zme/anaconda3/envs/Transformer/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Model Outputs:\n",
      "\n",
      "Example 0 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zme/anaconda3/envs/Transformer/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Text (Input)        : <s> Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen </s>\n",
      "Target Text (Ground Truth) : <s> A group of men are loading cotton onto a truck </s>\n",
      "Model Output               : <s> A group of men are picking up a truck while being pulled up on a truck . </s>\n",
      "\n",
      "Example 1 ========\n",
      "\n",
      "Source Text (Input)        : <s> Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen </s>\n",
      "Target Text (Ground Truth) : <s> A group of men are loading cotton onto a truck </s>\n",
      "Model Output               : <s> A group of men on a truck is hanging out of a truck . </s>\n",
      "\n",
      "Example 2 ========\n",
      "\n",
      "Source Text (Input)        : <s> Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen </s>\n",
      "Target Text (Ground Truth) : <s> A group of men are loading cotton onto a truck </s>\n",
      "Model Output               : <s> A group of men are spinning out of a truck 's hanging on a truck . </s>\n",
      "\n",
      "Example 3 ========\n",
      "\n",
      "Source Text (Input)        : <s> Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen </s>\n",
      "Target Text (Ground Truth) : <s> A group of men are loading cotton onto a truck </s>\n",
      "Model Output               : <s> A group of men are putting on a truck 's truck . </s>\n",
      "\n",
      "Example 4 ========\n",
      "\n",
      "Source Text (Input)        : <s> Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen </s>\n",
      "Target Text (Ground Truth) : <s> A group of men are loading cotton onto a truck </s>\n",
      "Model Output               : <s> A group of men are writing on a truck . </s>\n"
     ]
    }
   ],
   "source": [
    "def check_outputs(\n",
    "    valid_dataloader,\n",
    "    model,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    n_examples=15,\n",
    "    pad_idx=2,\n",
    "    eos_string=\"</s>\",\n",
    "):\n",
    "    results = [()] * n_examples\n",
    "    for idx in range(n_examples):\n",
    "        print(\"\\nExample %d ========\\n\" % idx)\n",
    "        b = next(iter(valid_dataloader))\n",
    "        rb = Batch(b[0], b[1], pad_idx)\n",
    "        greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\n",
    "\n",
    "        src_tokens = [\n",
    "            vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n",
    "        ]\n",
    "        tgt_tokens = [\n",
    "            vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            \"Source Text (Input)        : \"\n",
    "            + \" \".join(src_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        print(\n",
    "            \"Target Text (Ground Truth) : \"\n",
    "            + \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\n",
    "        model_txt = (\n",
    "            \" \".join(\n",
    "                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n",
    "            ).split(eos_string, 1)[0]\n",
    "            + eos_string\n",
    "        )\n",
    "        print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))\n",
    "        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_model_example(n_examples=5):\n",
    "    global vocab_de, vocab_en, spacy_de, spacy_en\n",
    "    config = {\n",
    "        'epochs': 8,\n",
    "        'model_prefix': 'multi30k',\n",
    "        'accum_interval': 10,\n",
    "        'batch_size': 1,\n",
    "        'warmup': 3000,\n",
    "        'base_lr': 1.0,\n",
    "        'max_seqlen': 72,\n",
    "    }\n",
    "\n",
    "    print(\"Preparing Data ...\")\n",
    "    _, valid_dataloader = create_dataloader(\n",
    "        config,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        vocab_de,\n",
    "        vocab_en,\n",
    "        torch.device(\"cpu\"),\n",
    "    )\n",
    "\n",
    "    print(\"Loading Trained Model ...\")\n",
    "\n",
    "    model = make_model(len(vocab_de), len(vocab_en), N=6, max_len=5000)\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"{}_final.pt\".format(config['model_prefix']), map_location=torch.device(\"cpu\"))\n",
    "    )\n",
    "\n",
    "    print(\"Checking Model Outputs:\")\n",
    "    example_data = check_outputs(\n",
    "        valid_dataloader, model, vocab_de, vocab_en, n_examples=n_examples\n",
    "    )\n",
    "    return model, example_data\n",
    "\n",
    "\n",
    "_ = run_model_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
